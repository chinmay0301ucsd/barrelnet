{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mmengine import Config, DictAction\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import visu3d as v3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_camera_model(H : int, W : int, intrinsics : list) -> np.array:\n",
    "    \"\"\"\n",
    "    Encode the camera intrinsic parameters (focal length and principle point) to a 4-channel map. \n",
    "    \"\"\"\n",
    "    fx, fy, u0, v0 = intrinsics\n",
    "    f = (fx + fy) / 2.0\n",
    "    # principle point location\n",
    "    x_row = np.arange(0, W).astype(np.float32)\n",
    "    x_row_center_norm = (x_row - u0) / W\n",
    "    x_center = np.tile(x_row_center_norm, (H, 1)) # [H, W]\n",
    "\n",
    "    y_col = np.arange(0, H).astype(np.float32) \n",
    "    y_col_center_norm = (y_col - v0) / H\n",
    "    y_center = np.tile(y_col_center_norm, (W, 1)).T # [H, W]\n",
    "\n",
    "    # FoV\n",
    "    fov_x = np.arctan(x_center / (f / W))\n",
    "    fov_y = np.arctan(y_center / (f / H))\n",
    "\n",
    "    cam_model = np.stack([x_center, y_center, fov_x, fov_y], axis=2)\n",
    "    return cam_model\n",
    "\n",
    "def resize_for_input(image, output_shape, intrinsic, canonical_shape, to_canonical_ratio):\n",
    "    \"\"\"\n",
    "    Resize the input.\n",
    "    Resizing consists of two processed, i.e. 1) to the canonical space (adjust the camera model); 2) resize the image while the camera model holds. Thus the\n",
    "    label will be scaled with the resize factor.\n",
    "    \"\"\"\n",
    "    padding = [123.675, 116.28, 103.53]\n",
    "    h, w, _ = image.shape\n",
    "    resize_ratio_h = output_shape[0] / canonical_shape[0]\n",
    "    resize_ratio_w = output_shape[1] / canonical_shape[1]\n",
    "    to_scale_ratio = min(resize_ratio_h, resize_ratio_w)\n",
    "\n",
    "    resize_ratio = to_canonical_ratio * to_scale_ratio\n",
    "\n",
    "    reshape_h = int(resize_ratio * h)\n",
    "    reshape_w = int(resize_ratio * w)\n",
    "\n",
    "    pad_h = max(output_shape[0] - reshape_h, 0)\n",
    "    pad_w = max(output_shape[1] - reshape_w, 0)\n",
    "    pad_h_half = int(pad_h / 2)\n",
    "    pad_w_half = int(pad_w / 2)\n",
    "\n",
    "    # resize\n",
    "    image = cv2.resize(image, dsize=(reshape_w, reshape_h), interpolation=cv2.INTER_LINEAR)\n",
    "    # padding\n",
    "    image = cv2.copyMakeBorder(\n",
    "        image, \n",
    "        pad_h_half, \n",
    "        pad_h - pad_h_half, \n",
    "        pad_w_half, \n",
    "        pad_w - pad_w_half, \n",
    "        cv2.BORDER_CONSTANT, \n",
    "        value=padding)\n",
    "    \n",
    "    # Resize, adjust principle point\n",
    "    intrinsic[2] = intrinsic[2] * to_scale_ratio\n",
    "    intrinsic[3] = intrinsic[3] * to_scale_ratio\n",
    "\n",
    "    cam_model = build_camera_model(reshape_h, reshape_w, intrinsic)\n",
    "    cam_model = cv2.copyMakeBorder(\n",
    "        cam_model, \n",
    "        pad_h_half, \n",
    "        pad_h - pad_h_half, \n",
    "        pad_w_half, \n",
    "        pad_w - pad_w_half, \n",
    "        cv2.BORDER_CONSTANT, \n",
    "        value=-1)\n",
    "\n",
    "    pad = [pad_h_half, pad_h - pad_h_half, pad_w_half, pad_w - pad_w_half]\n",
    "    label_scale_factor = 1 / to_scale_ratio\n",
    "    return image, cam_model, pad, label_scale_factor\n",
    "\n",
    "def transform_test_data_scalecano(rgb, intrinsic, data_basic):\n",
    "    \"\"\"\n",
    "    Pre-process the input for forwarding. Employ `label scale canonical transformation.'\n",
    "        Args:\n",
    "            rgb: input rgb image. [H, W, 3]\n",
    "            intrinsic: camera intrinsic parameter, [fx, fy, u0, v0]\n",
    "            data_basic: predefined canonical space in configs.\n",
    "    \"\"\"\n",
    "    canonical_space = data_basic[\"canonical_space\"]\n",
    "    forward_size = data_basic[\"crop_size\"]\n",
    "    mean = torch.tensor([123.675, 116.28, 103.53]).float()[:, None, None]\n",
    "    std = torch.tensor([58.395, 57.12, 57.375]).float()[:, None, None]\n",
    "\n",
    "    # BGR to RGB\n",
    "    # rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    ori_h, ori_w, _ = rgb.shape\n",
    "    ori_focal = (intrinsic[0] + intrinsic[1]) / 2\n",
    "    canonical_focal = canonical_space['focal_length']\n",
    "\n",
    "    cano_label_scale_ratio = canonical_focal / ori_focal\n",
    "\n",
    "    canonical_intrinsic = [\n",
    "        intrinsic[0] * cano_label_scale_ratio,\n",
    "        intrinsic[1] * cano_label_scale_ratio,\n",
    "        intrinsic[2],\n",
    "        intrinsic[3],\n",
    "    ]\n",
    "\n",
    "    # resize\n",
    "    rgb, cam_model, pad, resize_label_scale_ratio = resize_for_input(rgb, forward_size, canonical_intrinsic, [ori_h, ori_w], 1.0)\n",
    "\n",
    "    # label scale factor\n",
    "    label_scale_factor = cano_label_scale_ratio * resize_label_scale_ratio\n",
    "\n",
    "    rgb = torch.from_numpy(rgb.transpose((2, 0, 1))).float()\n",
    "    rgb = torch.div((rgb - mean), std)\n",
    "    rgb = rgb.cuda()\n",
    "    \n",
    "    cam_model = torch.from_numpy(cam_model.transpose((2, 0, 1))).float()\n",
    "    cam_model = cam_model[None, :, :, :].cuda()\n",
    "    cam_model_stacks = [\n",
    "        torch.nn.functional.interpolate(cam_model, size=(cam_model.shape[2]//i, cam_model.shape[3]//i), mode='bilinear', align_corners=False)\n",
    "        for i in [2, 4, 8, 16, 32]\n",
    "    ]\n",
    "    return rgb, cam_model_stacks, pad, label_scale_factor\n",
    "\n",
    "def align_scale(pred: torch.tensor, target: torch.tensor):\n",
    "    mask = target > 0\n",
    "    if torch.sum(mask) > 10:\n",
    "        scale = torch.median(target[mask]) / (torch.median(pred[mask]) + 1e-8)\n",
    "    else:\n",
    "        scale = 1\n",
    "    pred_scaled = pred * scale\n",
    "    return pred_scaled, scale\n",
    "\n",
    "def get_prediction(\n",
    "    model: torch.nn.Module,\n",
    "    input: torch.tensor,\n",
    "    cam_model: torch.tensor,\n",
    "    pad_info: torch.tensor,\n",
    "    scale_info: torch.tensor,\n",
    "    gt_depth: torch.tensor,\n",
    "    normalize_scale: float,\n",
    "    ori_shape: list=[],\n",
    "):\n",
    "\n",
    "    data = dict(\n",
    "        input=input,\n",
    "        cam_model=cam_model,\n",
    "    )\n",
    "    #pred_depth, confidence, output_dict = model.module.inference(data)\n",
    "    pred_depth, confidence, output_dict = model.inference(data)\n",
    "    pred_depth = pred_depth.squeeze()\n",
    "    pred_depth = pred_depth[pad_info[0] : pred_depth.shape[0] - pad_info[1], pad_info[2] : pred_depth.shape[1] - pad_info[3]]\n",
    "    confidence = confidence.squeeze()\n",
    "    confidence = confidence[pad_info[0] : confidence.shape[0] - pad_info[1], pad_info[2] : confidence.shape[1] - pad_info[3]]\n",
    "    if gt_depth is not None:\n",
    "        resize_shape = gt_depth.shape\n",
    "    elif ori_shape != []:\n",
    "        resize_shape = ori_shape\n",
    "    else:\n",
    "        resize_shape = pred_depth.shape\n",
    "\n",
    "    pred_depth = torch.nn.functional.interpolate(pred_depth[None, None, :, :], resize_shape, mode='bilinear').squeeze() # to original size\n",
    "    pred_depth = pred_depth * normalize_scale / scale_info\n",
    "    if gt_depth is not None:\n",
    "        pred_depth_scale, scale = align_scale(pred_depth, gt_depth)\n",
    "    else:\n",
    "        pred_depth_scale = None\n",
    "        scale = None\n",
    "\n",
    "    return pred_depth, pred_depth_scale, scale, output_dict, confidence\n",
    "\n",
    "def gray_to_colormap(img, cmap='rainbow'):\n",
    "    \"\"\"\n",
    "    Transfer gray map to matplotlib colormap\n",
    "    \"\"\"\n",
    "    assert img.ndim == 2\n",
    "\n",
    "    img[img<0] = 0\n",
    "    mask_invalid = img < 1e-10\n",
    "    img = img / (img.max() + 1e-8)\n",
    "    norm = matplotlib.colors.Normalize(vmin=0, vmax=1.1)\n",
    "    cmap_m = matplotlib.cm.get_cmap(cmap)\n",
    "    map = matplotlib.cm.ScalarMappable(norm=norm, cmap=cmap_m)\n",
    "    colormap = (map.to_rgba(img)[:, :, :3] * 255).astype(np.uint8)\n",
    "    colormap[mask_invalid] = 0\n",
    "    return colormap\n",
    "\n",
    "def vis_surface_normal(normal: torch.tensor, mask: torch.tensor=None) -> np.array:\n",
    "    \"\"\"\n",
    "    Visualize surface normal. Transfer surface normal value from [-1, 1] to [0, 255]\n",
    "    Aargs:\n",
    "        normal (torch.tensor, [h, w, 3]): surface normal\n",
    "        mask (torch.tensor, [h, w]): valid masks\n",
    "    \"\"\"\n",
    "    normal = normal.cpu().numpy().squeeze()\n",
    "    n_img_L2 = np.sqrt(np.sum(normal ** 2, axis=2, keepdims=True))\n",
    "    n_img_norm = normal / (n_img_L2 + 1e-8)\n",
    "    normal_vis = n_img_norm * 127\n",
    "    normal_vis += 128\n",
    "    normal_vis = normal_vis.astype(np.uint8)\n",
    "    if mask is not None:\n",
    "        mask = mask.cpu().numpy().squeeze()\n",
    "        normal_vis[~mask] = 0\n",
    "    return normal_vis\n",
    "\n",
    "class BaseDepthModel(nn.Module):\n",
    "    def __init__(self, cfg, **kwargs) -> None:\n",
    "        super(BaseDepthModel, self).__init__()\n",
    "        model_type = cfg.model.type\n",
    "        self.depth_model = get_func('mono.model.model_pipelines.' + model_type)(cfg)\n",
    "\n",
    "    def forward(self, data):\n",
    "        output = self.depth_model(**data)\n",
    "\n",
    "        return output['prediction'], output['confidence'], output\n",
    "\n",
    "    def inference(self, data):\n",
    "        with torch.no_grad():\n",
    "            pred_depth, confidence, _ = self.forward(data)\n",
    "        return pred_depth, confidence\n",
    "\n",
    "\n",
    "def get_func(func_name):\n",
    "    \"\"\"\n",
    "        Helper to return a function object by name. func_name must identify \n",
    "        a function in this module or the path to a function relative to the base\n",
    "        module.\n",
    "        @ func_name: function name.\n",
    "    \"\"\"\n",
    "    if func_name == '':\n",
    "        return None\n",
    "    try:\n",
    "        parts = func_name.split('.')\n",
    "        # Refers to a function in this module\n",
    "        if len(parts) == 1:\n",
    "            return globals()[parts[0]]\n",
    "        # Otherwise, assume we're referencing a module under modeling\n",
    "        module_name = '.'.join(parts[:-1])\n",
    "        module = importlib.import_module(module_name)\n",
    "        return getattr(module, parts[-1])\n",
    "    except:\n",
    "        raise RuntimeError(f'Failed to find function: {func_name}')\n",
    "\n",
    "class DepthModel(BaseDepthModel):\n",
    "    def __init__(self, cfg, **kwargs):\n",
    "        super(DepthModel, self).__init__(cfg)   \n",
    "        model_type = cfg.model.type\n",
    "        \n",
    "    def inference(self, data):\n",
    "        with torch.no_grad():\n",
    "            pred_depth, confidence, output_dict = self.forward(data)       \n",
    "        return pred_depth, confidence, output_dict\n",
    "\n",
    "def get_monodepth_model(\n",
    "    cfg : dict,\n",
    "    **kwargs\n",
    "    ) -> nn.Module:\n",
    "    # config depth  model\n",
    "    model = DepthModel(cfg, **kwargs)\n",
    "    #model.init_weights(load_imagenet_model, imagenet_ckpt_fpath)\n",
    "    assert isinstance(model, nn.Module)\n",
    "    return model\n",
    "\n",
    "def get_configured_monodepth_model(\n",
    "    cfg: dict,\n",
    "    ) -> nn.Module:\n",
    "    \"\"\"\n",
    "        Args:\n",
    "        @ configs: configures for the network.\n",
    "        @ load_imagenet_model: whether to initialize from ImageNet-pretrained model.\n",
    "        @ imagenet_ckpt_fpath: string representing path to file with weights to initialize model with.\n",
    "        Returns:\n",
    "        # model: depth model.\n",
    "    \"\"\"\n",
    "    model = get_monodepth_model(cfg)\n",
    "    return model\n",
    "\n",
    "def load_ckpt(load_path, model, optimizer=None, scheduler=None, strict_match=True, loss_scaler=None):\n",
    "    \"\"\"\n",
    "    Load the check point for resuming training or finetuning.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(load_path):\n",
    "        checkpoint = torch.load(load_path, map_location=\"cpu\")\n",
    "        ckpt_state_dict  = checkpoint['model_state_dict']\n",
    "        model.load_state_dict(ckpt_state_dict, strict=strict_match)\n",
    "\n",
    "        if optimizer is not None:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        if scheduler is not None:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        if loss_scaler is not None and 'scaler' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scaler'])\n",
    "        del ckpt_state_dict\n",
    "        del checkpoint\n",
    "    return model, optimizer, scheduler, loss_scaler\n",
    "\n",
    "def get_pcd_base(H, W, u0, v0, fx, fy):\n",
    "    x_row = np.arange(0, W)\n",
    "    x = np.tile(x_row, (H, 1))\n",
    "    x = x.astype(np.float32)\n",
    "    u_m_u0 = x - u0\n",
    "\n",
    "    y_col = np.arange(0, H)  # y_col = np.arange(0, height)\n",
    "    y = np.tile(y_col, (W, 1)).T\n",
    "    y = y.astype(np.float32)\n",
    "    v_m_v0 = y - v0\n",
    "\n",
    "    x = u_m_u0 / fx\n",
    "    y = v_m_v0 / fy\n",
    "    z = np.ones_like(x)\n",
    "    pw = np.stack([x, y, z], axis=2)  # [h, w, c]\n",
    "    return pw\n",
    "\n",
    "def reconstruct_pcd(depth, fx, fy, u0, v0, pcd_base=None, mask=None):\n",
    "    if type(depth) == torch.__name__:\n",
    "        depth = depth.cpu().numpy().squeeze()\n",
    "    depth = cv2.medianBlur(depth, 5)\n",
    "    if pcd_base is None:\n",
    "        H, W = depth.shape\n",
    "        pcd_base = get_pcd_base(H, W, u0, v0, fx, fy)\n",
    "    pcd = depth[:, :, None] * pcd_base\n",
    "    if mask:\n",
    "        pcd[mask] = 0\n",
    "    return pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config.fromfile('barrelnet/metric3d/vit.raft5.large.py')\n",
    "model = get_configured_monodepth_model(cfg)\n",
    "model, _,  _, _ = load_ckpt('checkpoints/metric_depth_vit_large_800k.pth', model, strict_match=False)\n",
    "# model = torch.hub.load(\"yvanyin/metric3d\", \"metric3d_vit_large\", pretrain=True)\n",
    "model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgpath = Path(\"data/barrel2/cropped0005.jpg\")\n",
    "# imgpath = Path(\"data/files_underwater.jpg\")\n",
    "fx = 1000.0\n",
    "fy = 1000.0\n",
    "img = cv2.imread(str(imgpath))\n",
    "cv_image = np.array(img)\n",
    "img = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\n",
    "intrinsic = [fx, fy, img.shape[1] / 2, img.shape[0] / 2]\n",
    "rgb_input, cam_models_stacks, pad, label_scale_factor = transform_test_data_scalecano(img, intrinsic, cfg.data_basic)\n",
    "with torch.no_grad():\n",
    "    pred_depth, pred_depth_scale, scale, output, confidence = get_prediction(\n",
    "        model = model,\n",
    "        input = rgb_input.unsqueeze(0),\n",
    "        cam_model = cam_models_stacks,\n",
    "        pad_info = pad,\n",
    "        scale_info = label_scale_factor,\n",
    "        gt_depth = None,\n",
    "        normalize_scale = cfg.data_basic.depth_range[1],\n",
    "        ori_shape=[img.shape[0], img.shape[1]],\n",
    "    )\n",
    "    pred_normal = output['normal_out_list'][0][:, :3, :, :] \n",
    "    H, W = pred_normal.shape[2:]\n",
    "    pred_normal = pred_normal[:, :, pad[0]:H-pad[1], pad[2]:W-pad[3]]\n",
    "\n",
    "pred_depth = pred_depth.squeeze().cpu().numpy()\n",
    "pred_depth[pred_depth<0] = 0\n",
    "pred_color = gray_to_colormap(pred_depth)\n",
    "\n",
    "pred_normal = torch.nn.functional.interpolate(pred_normal, [img.shape[0], img.shape[1]], mode='bilinear').squeeze()\n",
    "pred_normal = pred_normal.permute(1,2,0)\n",
    "pred_color_normal = vis_surface_normal(pred_normal)\n",
    "pred_normal = pred_normal.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pred_depth / np.max(pred_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pred_color_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = reconstruct_pcd(pred_depth, intrinsic[0], intrinsic[1], intrinsic[2], intrinsic[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3d.Point3d(p=pcd.reshape(-1, 3)).fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgpath = Path(\"data/barrel2/cropped0005.jpg\")\n",
    "intrinsic = [1250.0, 1250.0, 960.0, 437.5]  # [fx, fy, cx, cy]\n",
    "rgb_origin = cv2.cvtColor(cv2.imread(str(imgpath)), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#### adjust input size to fit pretrained model\n",
    "# keep ratio resize\n",
    "input_size = (616, 1064) # for vit model\n",
    "# input_size = (544, 1216) # for convnext model\n",
    "h, w = rgb_origin.shape[:2]\n",
    "scale = min(input_size[0] / h, input_size[1] / w)\n",
    "rgb = cv2.resize(rgb_origin, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_LINEAR)\n",
    "# remember to scale intrinsic, hold depth\n",
    "intrinsic = [intrinsic[0] * scale, intrinsic[1] * scale, intrinsic[2] * scale, intrinsic[3] * scale]\n",
    "# padding to input_size\n",
    "padding = [123.675, 116.28, 103.53]\n",
    "h, w = rgb.shape[:2]\n",
    "pad_h = input_size[0] - h\n",
    "pad_w = input_size[1] - w\n",
    "pad_h_half = pad_h // 2\n",
    "pad_w_half = pad_w // 2\n",
    "rgb = cv2.copyMakeBorder(rgb, pad_h_half, pad_h - pad_h_half, pad_w_half, pad_w - pad_w_half, cv2.BORDER_CONSTANT, value=padding)\n",
    "pad_info = [pad_h_half, pad_h - pad_h_half, pad_w_half, pad_w - pad_w_half]\n",
    "\n",
    "#### normalize\n",
    "mean = torch.tensor([123.675, 116.28, 103.53]).float()[:, None, None]\n",
    "std = torch.tensor([58.395, 57.12, 57.375]).float()[:, None, None]\n",
    "rgb = torch.from_numpy(rgb.transpose((2, 0, 1))).float()\n",
    "rgb = torch.div((rgb - mean), std)\n",
    "rgb = rgb[None, :, :, :].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda().eval()\n",
    "with torch.no_grad():\n",
    "    pred_depth, confidence, output_dict = model.inference({\"input\": rgb})\n",
    "# un pad\n",
    "pred_depth = pred_depth.squeeze()\n",
    "pred_depth = pred_depth[pad_info[0] : pred_depth.shape[0] - pad_info[1], pad_info[2] : pred_depth.shape[1] - pad_info[3]]\n",
    "\n",
    "# upsample to original size\n",
    "pred_depth = torch.nn.functional.interpolate(pred_depth[None, None, :, :], rgb_origin.shape[:2], mode=\"bilinear\").squeeze()\n",
    "#### de-canonical transform\n",
    "canonical_to_real_scale = intrinsic[0] / 1000.0 # 1000.0 is the focal length of canonical camera\n",
    "pred_depth = pred_depth * canonical_to_real_scale # now the depth is metric\n",
    "pred_depth = torch.clamp(pred_depth, 0, 300)\n",
    "# pred_normal = output_dict[\"prediction_normal\"][:, :3, :, :] # only available for Metric3Dv2 i.e., ViT models\n",
    "# normal_confidence = output_dict[\"prediction_normal\"][:, 3, :, :] # see https://arxiv.org/abs/2109.09881 for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = pred_depth.cpu().numpy()\n",
    "depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.arange(depths.shape[0]).astype(float), np.arange(depths.shape[1]).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array([xx.reshape(-1), yy.reshape(-1), depths.reshape(-1)]).T\n",
    "pc = v3d.Point3d(p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### prepare data\n",
    "rgb_file = 'data/kitti_demo/rgb/0000000050.png'\n",
    "depth_file = 'data/kitti_demo/depth/0000000050.png'\n",
    "intrinsic = [707.0493, 707.0493, 604.0814, 180.5066]\n",
    "gt_depth_scale = 256.0\n",
    "rgb_origin = cv2.imread(rgb_file)[:, :, ::-1]\n",
    "\n",
    "#### ajust input size to fit pretrained model\n",
    "# keep ratio resize\n",
    "input_size = (616, 1064) # for vit model\n",
    "# input_size = (544, 1216) # for convnext model\n",
    "h, w = rgb_origin.shape[:2]\n",
    "scale = min(input_size[0] / h, input_size[1] / w)\n",
    "rgb = cv2.resize(rgb_origin, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_LINEAR)\n",
    "# remember to scale intrinsic, hold depth\n",
    "intrinsic = [intrinsic[0] * scale, intrinsic[1] * scale, intrinsic[2] * scale, intrinsic[3] * scale]\n",
    "# padding to input_size\n",
    "padding = [123.675, 116.28, 103.53]\n",
    "h, w = rgb.shape[:2]\n",
    "pad_h = input_size[0] - h\n",
    "pad_w = input_size[1] - w\n",
    "pad_h_half = pad_h // 2\n",
    "pad_w_half = pad_w // 2\n",
    "rgb = cv2.copyMakeBorder(rgb, pad_h_half, pad_h - pad_h_half, pad_w_half, pad_w - pad_w_half, cv2.BORDER_CONSTANT, value=padding)\n",
    "pad_info = [pad_h_half, pad_h - pad_h_half, pad_w_half, pad_w - pad_w_half]\n",
    "\n",
    "#### normalize\n",
    "mean = torch.tensor([123.675, 116.28, 103.53]).float()[:, None, None]\n",
    "std = torch.tensor([58.395, 57.12, 57.375]).float()[:, None, None]\n",
    "rgb = torch.from_numpy(rgb.transpose((2, 0, 1))).float()\n",
    "rgb = torch.div((rgb - mean), std)\n",
    "rgb = rgb[None, :, :, :].cuda()\n",
    "\n",
    "###################### canonical camera space ######################\n",
    "# inference\n",
    "model = torch.hub.load('yvanyin/metric3d', 'metric3d_vit_small', pretrain=True)\n",
    "model.cuda().eval()\n",
    "with torch.no_grad():\n",
    "    pred_depth, confidence, output_dict = model.inference({'input': rgb})\n",
    "\n",
    "# un pad\n",
    "pred_depth = pred_depth.squeeze()\n",
    "pred_depth = pred_depth[pad_info[0] : pred_depth.shape[0] - pad_info[1], pad_info[2] : pred_depth.shape[1] - pad_info[3]]\n",
    "\n",
    "# upsample to original size\n",
    "pred_depth = torch.nn.functional.interpolate(pred_depth[None, None, :, :], rgb_origin.shape[:2], mode='bilinear').squeeze()\n",
    "###################### canonical camera space ######################\n",
    "\n",
    "#### de-canonical transform\n",
    "canonical_to_real_scale = intrinsic[0] / 1000.0 # 1000.0 is the focal length of canonical camera\n",
    "pred_depth = pred_depth * canonical_to_real_scale # now the depth is metric\n",
    "pred_depth = torch.clamp(pred_depth, 0, 300)\n",
    "\n",
    "#### you can now do anything with the metric depth \n",
    "# such as evaluate predicted depth\n",
    "if depth_file is not None:\n",
    "    gt_depth = cv2.imread(depth_file, -1)\n",
    "    gt_depth = gt_depth / gt_depth_scale\n",
    "    gt_depth = torch.from_numpy(gt_depth).float().cuda()\n",
    "    assert gt_depth.shape == pred_depth.shape\n",
    "    \n",
    "    mask = (gt_depth > 1e-8)\n",
    "    abs_rel_err = (torch.abs(pred_depth[mask] - gt_depth[mask]) / gt_depth[mask]).mean()\n",
    "    print('abs_rel_err:', abs_rel_err.item())\n",
    "\n",
    "#### normal are also available\n",
    "if 'prediction_normal' in output_dict: # only available for Metric3Dv2, i.e. vit model\n",
    "    pred_normal = output_dict['prediction_normal'][:, :3, :, :]\n",
    "    normal_confidence = output_dict['prediction_normal'][:, 3, :, :] # see https://arxiv.org/abs/2109.09881 for details\n",
    "    # un pad and resize to some size if needed\n",
    "    pred_normal = pred_normal.squeeze()\n",
    "    pred_normal = pred_normal[:, pad_info[0] : pred_normal.shape[1] - pad_info[1], pad_info[2] : pred_normal.shape[2] - pad_info[3]]\n",
    "    # you can now do anything with the normal\n",
    "    # such as visualize pred_normal\n",
    "    pred_normal_vis = pred_normal.cpu().numpy().transpose((1, 2, 0))\n",
    "    pred_normal_vis = (pred_normal_vis + 1) / 2\n",
    "    cv2.imwrite('normal_vis.png', (pred_normal_vis * 255).astype(np.uint8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "barrels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
